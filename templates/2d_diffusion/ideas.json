[
    {
        "Name": "learning_rate_schedule",
        "Title": "Adaptive Learning Rate Schedules: Comparing different learning rate schedules for diffusion models.",
        "Experiment": "In this experiment, we compare the performance of different learning rate schedules on diffusion model performance. We use the final estimated KL as the evaluation metric.",
        "Interestingness": 4,
        "Feasibility": 10,
        "Novelty": 3,
        "novel": true
    },
    {
        "Name": "gaussian_random_feature_embeddings",
        "Title": "Exploration of Gaussian Random Feature Embeddings in Diffusion Models for Enhanced Data Representation",
        "Experiment": "Replace the sinusoidal embeddings in the MLPDenoiser with Gaussian Random Feature (GRF) embeddings. Implement a new class for GRF embeddings that generates random Fourier features as described in Rahimi and Recht's 'Random Features for Large-Scale Kernel Machines'. Modify the MLPDenoiser to use these embeddings for both time and input. Evaluate the model's performance in terms of KL divergence and mode coverage, and compare against the original sinusoidal embeddings.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": true
    },
    {
        "Name": "attention_mechanism_integration",
        "Title": "Enhancing Diffusion Models with Attention Mechanisms for Improved Controllable Generation",
        "Experiment": "Integrate an attention mechanism within the MLPDenoiser. Add a self-attention layer that processes the concatenated sinusoidal embeddings before they are fed into the network. This layer will help the model to dynamically focus on different modes of the data during generation. Compare the performance in terms of KL divergence, mode coverage, and diversity of generated samples against the baseline model without attention.",
        "Interestingness": 9,
        "Feasibility": 6,
        "Novelty": 8,
        "novel": true
    }
]