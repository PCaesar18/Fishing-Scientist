[
    {
        "Name": "batch_size_grokking",
        "Title": "Batch Size Grokking: Assessing the impact of the training batchsize on the grokking phenomenon",
        "Experiment": "Modify the experiments to dynamically adjust the batch size during training, starting with a small batch size and gradually increasing it. This could potentially lead to faster generalization on the validation set.",
        "Interestingness": 6,
        "Feasibility": 4,
        "Novelty": 4,
        "novel": true
    },
    {
        "Name": "operation_complexity",
        "Title": "Operation Complexity and Its Impact on Grokking in Neural Networks",
        "Experiment": "Introduce new dataset classes for complex operations such as modular exponentiation and composite operations (e.g., (a + b) * c % p). Implement these in the AbstractDataset class and extend the get_data function to handle these new datasets. Run training and evaluation as before, and compare the performance metrics (accuracy, training loss, validation loss) and grokking patterns (time to achieve 99% validation accuracy) across different operations.",
        "Interestingness": 8,
        "Feasibility": 7,
        "Novelty": 7,
        "novel": true
    },
    {
        "Name": "dropout_grokking",
        "Title": "The Impact of Dropout on Grokking in Neural Networks",
        "Experiment": "Extend the Transformer class to include dropout layers after each self-attention and feedforward block. Modify the run function to include dropout as a hyperparameter with various dropout rates (e.g., 0.1, 0.3, 0.5). Run the experiments with different dropout rates across different datasets and compare the performance metrics and grokking patterns.",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    }
]